{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //a[@class=\"JtKRv\"] - XPath for A tag with link and text\n",
    "# //time[@class=\"hvbAAd\"] - XPath for Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_name = \"Apple\"\n",
    "# site_name = \"bbc\"\n",
    "# num_days_ago = \"1\"\n",
    "\n",
    "# # URL to scrape\n",
    "# url = f'https://news.google.com/search?q={company_name}%20site%3A{site_name}.com%20when%3A{num_days_ago}d&hl=en-GB&gl=GB&ceid=GB%3Aen'\n",
    "# driver.get(url)\n",
    "\n",
    "# # Find elements by xpaths\n",
    "# article_elements = driver.find_elements(By.XPATH, '//a[@class=\"JtKRv\"]')\n",
    "# datetime_elements = driver.find_elements(By.XPATH, '//time[@class=\"hvbAAd\"]')\n",
    "\n",
    "# # Data lists\n",
    "# data = []\n",
    "\n",
    "# # Check if the number of articles and datetimes are the same\n",
    "# if len(article_elements) == len(datetime_elements):\n",
    "#     for article, datetime_element in zip(article_elements, datetime_elements):\n",
    "#         # Extracting the datetime string and parsing it\n",
    "#         datetime_str = datetime_element.get_attribute('datetime')\n",
    "#         parsed_datetime = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "#         date = parsed_datetime.date()\n",
    "#         time = parsed_datetime.time()\n",
    "        \n",
    "#         # Extract article name and link\n",
    "#         article_name = article.text\n",
    "#         article_link = article.get_attribute('href')\n",
    "\n",
    "#         # Append to data list\n",
    "#         data.append([date, time, article_name, article_link, site_name])\n",
    "\n",
    "# # Create a DataFrame\n",
    "# columns = ['Date', 'Time', 'Article Name', 'Article Link', 'Site Name']\n",
    "# df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()\n",
    "\n",
    "# Output DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Article Name</th>\n",
       "      <th>Article Link</th>\n",
       "      <th>Site Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>Goldman Is Looking for a Way Out of Its Partne...</td>\n",
       "      <td>https://news.google.com/articles/CBMiZGh0dHBzO...</td>\n",
       "      <td>wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>An apple caramel pecan pie born from a railway...</td>\n",
       "      <td>https://news.google.com/articles/CBMiaGh0dHBzO...</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>Apple’s Next Killer App Is…Voicemail?</td>\n",
       "      <td>https://news.google.com/articles/CBMiS2h0dHBzO...</td>\n",
       "      <td>wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>Apple slams UK surveillance-bill proposals</td>\n",
       "      <td>https://news.google.com/articles/CBMiLGh0dHBzO...</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>Amazon Wants You to Pay With Your Palm. It's a...</td>\n",
       "      <td>https://news.google.com/articles/CBMicmh0dHBzO...</td>\n",
       "      <td>wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>09:30:00</td>\n",
       "      <td>Chinese consumers love AI smartphones but Appl...</td>\n",
       "      <td>https://news.google.com/articles/CBMiVmh0dHBzO...</td>\n",
       "      <td>finance.yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>15:50:02</td>\n",
       "      <td>Is Apple Stock Going to $240? 1 Wall Street An...</td>\n",
       "      <td>https://news.google.com/articles/CBMiRWh0dHBzO...</td>\n",
       "      <td>finance.yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>17:08:00</td>\n",
       "      <td>Apple Loop: iPhone 16 Design Leaks, New M4 Mac...</td>\n",
       "      <td>https://news.google.com/articles/CBMigAFodHRwc...</td>\n",
       "      <td>forbes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>What's News in Markets: Nvidia Effect, S&amp;P Mil...</td>\n",
       "      <td>https://news.google.com/articles/CBMijQFodHRwc...</td>\n",
       "      <td>wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>What's News in Markets: Nvidia Effect, S&amp;P Mil...</td>\n",
       "      <td>https://news.google.com/articles/CBMilQFodHRwc...</td>\n",
       "      <td>wsj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Time                                       Article Name  \\\n",
       "102  2023-06-30  07:00:00  Goldman Is Looking for a Way Out of Its Partne...   \n",
       "39   2023-07-01  07:00:00  An apple caramel pecan pie born from a railway...   \n",
       "83   2023-07-19  07:00:00              Apple’s Next Killer App Is…Voicemail?   \n",
       "30   2023-07-20  07:00:00         Apple slams UK surveillance-bill proposals   \n",
       "118  2023-08-11  07:00:00  Amazon Wants You to Pay With Your Palm. It's a...   \n",
       "..          ...       ...                                                ...   \n",
       "265  2024-06-22  09:30:00  Chinese consumers love AI smartphones but Appl...   \n",
       "266  2024-06-22  15:50:02  Is Apple Stock Going to $240? 1 Wall Street An...   \n",
       "159  2024-06-22  17:08:00  Apple Loop: iPhone 16 Design Leaks, New M4 Mac...   \n",
       "41   2024-06-22  10:00:00  What's News in Markets: Nvidia Effect, S&P Mil...   \n",
       "52   2024-06-22  10:00:00  What's News in Markets: Nvidia Effect, S&P Mil...   \n",
       "\n",
       "                                          Article Link      Site Name  \n",
       "102  https://news.google.com/articles/CBMiZGh0dHBzO...            wsj  \n",
       "39   https://news.google.com/articles/CBMiaGh0dHBzO...            bbc  \n",
       "83   https://news.google.com/articles/CBMiS2h0dHBzO...            wsj  \n",
       "30   https://news.google.com/articles/CBMiLGh0dHBzO...            bbc  \n",
       "118  https://news.google.com/articles/CBMicmh0dHBzO...            wsj  \n",
       "..                                                 ...            ...  \n",
       "265  https://news.google.com/articles/CBMiVmh0dHBzO...  finance.yahoo  \n",
       "266  https://news.google.com/articles/CBMiRWh0dHBzO...  finance.yahoo  \n",
       "159  https://news.google.com/articles/CBMigAFodHRwc...         forbes  \n",
       "41   https://news.google.com/articles/CBMijQFodHRwc...            wsj  \n",
       "52   https://news.google.com/articles/CBMilQFodHRwc...            wsj  \n",
       "\n",
       "[305 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup the Chrome WebDriver\n",
    "chromedriver_path = ChromeDriverManager().install()\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "company_name = \"Apple\"\n",
    "site_names = [\"bbc\", \"wsj\", \"nytimes\", \"forbes\", \"finance.yahoo\", \"bloomberg\", \"ft\"]\n",
    "num_ago = \"1\"\n",
    "time_ago = \"y\"  # for year or \"d\" for days\n",
    "\n",
    "# Data lists\n",
    "data = []\n",
    "\n",
    "# Loop over each site\n",
    "for site_name in site_names:\n",
    "    # URL to scrape\n",
    "    url = f'https://news.google.com/search?q={company_name}%20site%3A{site_name}.com%20when%3A{num_ago}{time_ago}&hl=en-GB&gl=GB&ceid=GB%3Aen'\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom to load all articles\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        driver.implicitly_wait(10)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Find elements by xpaths\n",
    "    article_elements = driver.find_elements(By.XPATH, '//a[@class=\"JtKRv\"]')\n",
    "    datetime_elements = driver.find_elements(By.XPATH, '//time[@class=\"hvbAAd\"]')\n",
    "\n",
    "    # Check if the number of articles and datetimes are the same\n",
    "    if len(article_elements) == len(datetime_elements):\n",
    "        for article, datetime_element in zip(article_elements, datetime_elements):\n",
    "            article_name = article.text\n",
    "\n",
    "            # Check if the article title contains the company name\n",
    "            if company_name.lower() in article_name.lower():\n",
    "                # Extracting the datetime string and parsing it\n",
    "                datetime_str = datetime_element.get_attribute('datetime')\n",
    "                parsed_datetime = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                date = parsed_datetime.date()\n",
    "                time = parsed_datetime.time()\n",
    "\n",
    "                # Extract article link\n",
    "                article_link = article.get_attribute('href')\n",
    "\n",
    "                # Append to data list\n",
    "                data.append([date, time, article_name, article_link, site_name])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['Date', 'Time', 'Article Name', 'Article Link', 'Site Name']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Sort and remove duplicates\n",
    "df = df.sort_values(by=[\"Date\", \"Site Name\"])\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Output DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"News Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Setup MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"news_articles_db\"]\n",
    "\n",
    "# List of companies and sites\n",
    "companies = [\"Apple\", \"Microsoft\", \"Google\", \"Amazon\", \"Nvidia\", \"Meta\", \"Tesla\"]\n",
    "site_names = [\"bbc\", \"wsj\", \"nytimes\", \"forbes\", \"finance.yahoo\", \"bloomberg\", \"ft\"]\n",
    "num_ago = \"1\"\n",
    "time_ago = \"y\" # for year or \"d\" for days\n",
    "\n",
    "# Setup the Chrome WebDriver\n",
    "chromedriver_path = ChromeDriverManager().install()\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "for company_name in companies:\n",
    "    for site_name in site_names:\n",
    "        # URL to scrape\n",
    "        url = f'https://news.google.com/search?q={company_name}%20site%3A{site_name}.com%20when%3A{num_ago}{time_ago}&hl=en-GB&gl=GB&ceid=GB%3Aen'\n",
    "        driver.get(url)\n",
    "\n",
    "        # Implement pagination\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load page\n",
    "            driver.implicitly_wait(10)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        # Find elements by xpaths\n",
    "        article_elements = driver.find_elements(By.XPATH, '//a[@class=\"JtKRv\"]')\n",
    "        datetime_elements = driver.find_elements(By.XPATH, '//time[@class=\"hvbAAd\"]')\n",
    "\n",
    "        # Data collection\n",
    "        data = []\n",
    "        if len(article_elements) == len(datetime_elements):\n",
    "            for article, datetime_element in zip(article_elements, datetime_elements):\n",
    "                article_name = article.text\n",
    "\n",
    "                # Check if the article title contains the company name\n",
    "                if company_name.lower() in article_name.lower():\n",
    "                    # Extracting the datetime string and parsing it\n",
    "                    datetime_str = datetime_element.get_attribute('datetime')\n",
    "                    if datetime_str.endswith('Z'):\n",
    "                        datetime_str = datetime_str[:-1]\n",
    "                    parsed_datetime = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                    date_time = parsed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    \n",
    "                    # Extract article link\n",
    "                    article_link = article.get_attribute('href')\n",
    "            \n",
    "                    # Append to data list\n",
    "                    data.append({\n",
    "                        \"DateTime\": date_time,\n",
    "                        \"Article Name\": article_name,\n",
    "                        \"Article Link\": article_link,\n",
    "                        \"Site Name\": site_name\n",
    "                    })\n",
    "\n",
    "        # Insert data into MongoDB\n",
    "        if data:\n",
    "            collection = db[company_name]\n",
    "            # Remove duplicates and insert\n",
    "            keys = {\"Article Link\": {\"$exists\": True}}\n",
    "            collection.create_index([(\"Article Link\", 1)], unique=True)\n",
    "            for item in data:\n",
    "                collection.update_one({\"Article Link\": item[\"Article Link\"]}, {'$setOnInsert': item}, upsert=True)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
